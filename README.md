# ultrasound-to-speech-project
Utilizing dilated, deformable convolution layers for ultrasound-to-mel  mapping towards ultrasound-to-speech synthesis 
Articulation-to-speech synthesis aims to generate natural and intelligible speech from articulatory data, offering   applications in silent speech interfaces. One promising modality for capturing articulatory movements is ultrasound tongue imaging (UTI), which provides non-invasive, high-frame-rate visualizations of tongue dynamics during speech production.  
Existing ultrasound-to-speech synthesis systems have two-step generation process. Firstly, an intermediate representation of speech is obtained from ultrasound tongue image frames (UTIF) sequence. Secondly, to obtain an audio waveform from the intermediate representation, a neural vocoder is employed.  
As intermediate representation, in research, mel-spectrogram of audio recordings are widely used. Csapó et al. utilized two-dimensional convolution neural networks to find the mapping between UTIF and 80-dimensional mel-spectrogram and WaveGlow vocoder for speech synthesis step. According to the results of this work, we observe that further exploration of different architectures is needed to obtain the best possible mapping which will provide more reliable intermediate representation for UTS system.  
To address this issue, in this project laboratory work, two  different convolutional layers (dilated, deformable) will be used to create model architectures for ultrasound-to-mel mapping task towards UTS system. Dilated convolutional layers are a type of convolution that “inflate” the kernel by inserting holes between the kernel elements. Deformable convolutions add 2D offsets to the regular grid sampling locations in the standard convolution. A transposed convolution will reverse the spatial transformation of a regular convolution with the same parameters. 
For the second step of UTS system, a HiFi-GAN neural vocoder will be employed for obtaining synthesized speech from generated mel-spectrograms.  
To evaluate the speaker-specific UTS system, results of objective metrics will be obtained.  For ultrasound-to-mel mapping step, Mean-Squared Error (MSE) and for synthesis step, Mel-Cepstral Distortion (MCD) will be calculated to compare the results of proposed models against our baseline (2D-CNN).  

